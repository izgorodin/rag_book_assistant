"""
RAG (Retrieval-Augmented Generation) Module

This module implements the final stage of the RAG system for question answering based on a given text corpus.
It utilizes the results from text processing and hybrid search to generate accurate and contextually relevant
answers to user queries.

Key components of the overall RAG system:
1. Text processing (src/text_processing.py):
   - Processing large files
   - Splitting text into chunks
   - Extracting dates, named entities, and key phrases
2. Hybrid search (src/hybrid_search.py):
   - Combining semantic (embedding-based) and lexical (BM25) search
   - Query expansion using synonyms
   - Weighting query tokens based on their parts of speech
3. RAG query processing (this module):
   - Using HybridSearch to find relevant chunks
   - Constructing context from relevant chunks and metadata
   - Generating answers using OpenAI's GPT model

The main function, rag_query, orchestrates the process from relevant chunk retrieval to answer generation.
"""

import re
from typing import List, Dict, Any
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from src.openai_service import OpenAIService
from src.logger import setup_logger
from src.book_data_interface import BookDataInterface
from src.search import CosineSearch, get_search_strategy
from src.error_handler import handle_rag_error
from src.embedding import EmbeddingService

logger = setup_logger()

@handle_rag_error
def preprocess_text(text: str) -> str:
    """
    Preprocess the input text by converting to lowercase, removing punctuation,
    tokenizing, removing stop words, and lemmatizing.

    Args:
        text (str): The input text to preprocess.

    Returns:
        str: The preprocessed text.
    """
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return ' '.join(tokens)


@handle_rag_error
def rag_query(query: str, book_data: BookDataInterface, openai_service: OpenAIService, 
             embedding_service: EmbeddingService) -> str:
    """Process query using RAG approach."""
    try:
        # Создаем поисковую стратегию с embedding_service
        search_strategy = CosineSearch(book_data, embedding_service)
        relevant_chunks = search_strategy.search(query, top_k=3)
        
        # Формируем контекст из найденных чанков
        context = " ".join(chunk['chunk'] for chunk in relevant_chunks)
        
        # Генерируем ответ
        return openai_service.generate_answer(query, context)
    except Exception as e:
        logger.error(f"Error in rag_query: {str(e)}")
        return f"Sorry, I encountered an error while processing your request: {str(e)}"

@handle_rag_error
def evaluate_answer_quality(generated_answer: str, reference_answer: str) -> float:
    """
    Evaluate the quality of a generated answer against a reference answer.

    Args:
        generated_answer (str): The answer generated by the model.
        reference_answer (str): The correct or expected answer for comparison.

    Returns:
        float: A score between 0 and 1 indicating the quality of the generated answer,
               where 1 means a perfect match and 0 means no similarity.
    """
    # Simple evaluation logic based on string similarity
    if not generated_answer or not reference_answer:
        return 0.0  # Return 0 if either answer is empty

    # Normalize answers by converting to lowercase
    generated_answer = generated_answer.lower()
    reference_answer = reference_answer.lower()

    # Calculate similarity score (this is a placeholder for a more complex logic)
    score = 1.0 if generated_answer == reference_answer else 0.0

    return score

@handle_rag_error
def get_answer_from_system(query: str, book_data: BookDataInterface, openai_service: OpenAIService) -> str:
    return rag_query(query, book_data, openai_service)

def format_context(chunks: List[str]) -> str:
    """Форматирует найденные чанки в контект для запроса."""
    return "\n\n".join(f"[{i+1}] {chunk}" for i, chunk in enumerate(chunks))
