"""
RAG (Retrieval-Augmented Generation) Module

This module implements the final stage of the RAG system for question answering based on a given text corpus.
It utilizes the results from text processing and hybrid search to generate accurate and contextually relevant
answers to user queries.

Key components of the overall RAG system:
1. Text processing (src/text_processing.py):
   - Processing large files
   - Splitting text into chunks
   - Extracting dates, named entities, and key phrases
2. Hybrid search (src/hybrid_search.py):
   - Combining semantic (embedding-based) and lexical (BM25) search
   - Query expansion using synonyms
   - Weighting query tokens based on their parts of speech
3. RAG query processing (this module):
   - Using HybridSearch to find relevant chunks
   - Constructing context from relevant chunks and metadata
   - Generating answers using OpenAI's GPT model

The main function, rag_query, orchestrates the process from relevant chunk retrieval to answer generation.
"""

import re
from typing import List, Dict, Any
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from src.openai_service import OpenAIService
from src.utils.logger import get_main_logger, get_rag_logger
from src.book_data_interface import BookDataInterface
from src.search import CosineSearch, get_search_strategy
from src.utils.error_handler import handle_rag_error
from src.embedding import EmbeddingService
from src.config import TOP_K_CHUNKS

# Initialize main and RAG-specific loggers
logger = get_main_logger()
rag_logger = get_rag_logger()

@handle_rag_error
def rag_query(query: str, book_data: BookDataInterface, openai_service: OpenAIService) -> str:
    """Process query using RAG approach."""
    try:
        relevant_chunks = book_data.get_relevant_chunks(query)
        context = "\n\n".join(relevant_chunks)
        return openai_service.generate_answer(query, context)
    except Exception as e:
        logger.error(f"Error in rag_query: {str(e)}")
        return f"Error processing query: {str(e)}"

@handle_rag_error
def evaluate_answer_quality(generated_answer: str, reference_answer: str) -> float:
    """
    Evaluate the quality of a generated answer against a reference answer.

    Args:
        generated_answer (str): The answer generated by the model.
        reference_answer (str): The correct or expected answer for comparison.

    Returns:
        float: A score between 0 and 1 indicating the quality of the generated answer,
               where 1 means a perfect match and 0 means no similarity.
    """
    # Simple evaluation logic based on string similarity
    if not generated_answer or not reference_answer:
        return 0.0  # Return 0 if either answer is empty

    # Normalize answers by converting to lowercase
    generated_answer = generated_answer.lower()
    reference_answer = reference_answer.lower()

    # Calculate similarity score (this is a placeholder for a more complex logic)
    score = 1.0 if generated_answer == reference_answer else 0.0

    return score

@handle_rag_error
def get_answer_from_system(query: str, book_data: BookDataInterface, openai_service: OpenAIService) -> str:
    """Retrieve an answer from the RAG system based on the provided query."""
    return rag_query(query, book_data, openai_service)

def format_context(chunks: List[str]) -> str:
    """Format the found chunks into context for the query."""
    return "\n\n".join(f"[{i+1}] {chunk}" for i, chunk in enumerate(chunks))
